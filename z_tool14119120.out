‚úèÔ∏è Setting installed package as editable
Running tool from Rust!

Tool run implementation called with 1 configs and  1 GPUs.
Loaded environment variables from .env file.
Starting tool run with 1 configs.
Processing config: ToolConfig { model: Local(meta-llama/Llama-3.1-8B-Instruct), translate_mode: NotTranslated, add_noise_mode: NoNoise }
Skipping question translation (pre-translate not enabled)
All test cases for meta-llama/Llama-3.1-8B-Instruct have already been processed. Skipping model loading and inference.
Skipping answer translation (post-translate not enabled)
Evaluating 200 cases...
Sorting evaluation results.
Sorted evaluation results. Writing to file.
Scoring evaluation results...
Score result written to tool/result/score/meta-llama-Llama-3.1-8B-Instruct/_en_na_nopretrans_nonoise_noprompt_noposttrans.jsonl: EvaluationSummary { accuracy: 0.0, total_cases: 200, correct_cases: 0 }
Categorizing 200 error samples...
Acquiring lock for category cache file...
Acquired lock for category cache file.
Loading category cache from tool_category_cache.jsonl...
Global backend not available, or model mismatch. Creating new backend for model Local(meta-llama/Llama-3.1-8B-Instruct)...
Building Rust extension with maturin develop...
Loading configs from: tool_config_slurm.py
Creating vLLM backend for model meta-llama/Llama-3.1-8B-Instruct with 1 GPUs...
INFO 12-16 11:14:49 [model.py:637] Resolved architecture: LlamaForCausalLM
INFO 12-16 11:14:49 [model.py:1750] Using max model len 131072
INFO 12-16 11:14:54 [scheduler.py:228] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(EngineCore_DP0 pid=1957910)[0;0m INFO 12-16 11:14:55 [core.py:93] Initializing a V1 LLM engine (v0.12.0) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01), seed=0, served_model_name=meta-llama/Llama-3.1-8B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 256, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=1957910)[0;0m INFO 12-16 11:15:16 [parallel_state.py:1200] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://141.142.254.17:40145 backend=nccl
[0;36m(EngineCore_DP0 pid=1957910)[0;0m INFO 12-16 11:15:18 [parallel_state.py:1408] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=1957910)[0;0m INFO 12-16 11:15:19 [gpu_model_runner.py:3467] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
[0;36m(EngineCore_DP0 pid=1957910)[0;0m INFO 12-16 11:15:27 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=1957910)[0;0m INFO 12-16 11:15:42 [default_loader.py:308] Loading weights took 14.26 seconds
[0;36m(EngineCore_DP0 pid=1957910)[0;0m INFO 12-16 11:15:43 [gpu_model_runner.py:3549] Model loading took 14.9889 GiB memory and 22.252944 seconds
[0;36m(EngineCore_DP0 pid=1957910)[0;0m INFO 12-16 11:16:00 [backends.py:655] Using cache directory: /u/zluo8/.cache/vllm/torch_compile_cache/ebb0ac8eeb/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=1957910)[0;0m INFO 12-16 11:16:00 [backends.py:715] Dynamo bytecode transform time: 15.96 s
[0;36m(EngineCore_DP0 pid=1957910)[0;0m INFO 12-16 11:16:04 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 3.396 s
[0;36m(EngineCore_DP0 pid=1957910)[0;0m INFO 12-16 11:16:10 [monitor.py:34] torch.compile takes 19.36 s in total
[0;36m(EngineCore_DP0 pid=1957910)[0;0m INFO 12-16 11:16:13 [gpu_worker.py:359] Available KV cache memory: 19.91 GiB
[0;36m(EngineCore_DP0 pid=1957910)[0;0m INFO 12-16 11:16:13 [kv_cache_utils.py:1286] GPU KV cache size: 163,104 tokens
[0;36m(EngineCore_DP0 pid=1957910)[0;0m INFO 12-16 11:16:13 [kv_cache_utils.py:1291] Maximum concurrency for 131,072 tokens per request: 1.24x
[0;36m(EngineCore_DP0 pid=1957910)[0;0m INFO 12-16 11:16:20 [gpu_model_runner.py:4466] Graph capturing finished in 6 secs, took 0.37 GiB
[0;36m(EngineCore_DP0 pid=1957910)[0;0m INFO 12-16 11:16:20 [core.py:254] init engine (profile, create kv cache, warmup model) took 36.06 seconds
[1/200] Categorized error for case multiple_0
[2/200] Categorized error for case multiple_1
[3/200] Categorized error for case multiple_2
[4/200] Categorized error for case multiple_3
[5/200] Categorized error for case multiple_4
[6/200] Categorized error for case multiple_5
[7/200] Categorized error for case multiple_6
[8/200] Categorized error for case multiple_7
[9/200] Categorized error for case multiple_8
[10/200] Categorized error for case multiple_9
[11/200] Categorized error for case multiple_10
[12/200] Categorized error for case multiple_11
[13/200] Categorized error for case multiple_12
[14/200] Categorized error for case multiple_13
[15/200] Categorized error for case multiple_14
[16/200] Categorized error for case multiple_15
[17/200] Categorized error for case multiple_16
[18/200] Categorized error for case multiple_17
[19/200] Categorized error for case multiple_18
[20/200] Categorized error for case multiple_19
[21/200] Categorized error for case multiple_20
[22/200] Categorized error for case multiple_21
[23/200] Categorized error for case multiple_22
[24/200] Categorized error for case multiple_23
[25/200] Categorized error for case multiple_24
[26/200] Categorized error for case multiple_25
[27/200] Categorized error for case multiple_26
[28/200] Categorized error for case multiple_27
[29/200] Categorized error for case multiple_28
[30/200] Categorized error for case multiple_29
[31/200] Categorized error for case multiple_30
[32/200] Categorized error for case multiple_31
[33/200] Categorized error for case multiple_32
[34/200] Categorized error for case multiple_33
[35/200] Categorized error for case multiple_34
[36/200] Categorized error for case multiple_35
[37/200] Categorized error for case multiple_36
[38/200] Categorized error for case multiple_37
[39/200] Categorized error for case multiple_38
[40/200] Categorized error for case multiple_39
[41/200] Categorized error for case multiple_40
[42/200] Categorized error for case multiple_41
[43/200] Categorized error for case multiple_42
[44/200] Categorized error for case multiple_43
[45/200] Categorized error for case multiple_44
[46/200] Categorized error for case multiple_45
[47/200] Categorized error for case multiple_46
[48/200] Categorized error for case multiple_47
[49/200] Categorized error for case multiple_48
[50/200] Categorized error for case multiple_49
[51/200] Categorized error for case multiple_50
[52/200] Categorized error for case multiple_51
[53/200] Categorized error for case multiple_52
[54/200] Categorized error for case multiple_53
[55/200] Categorized error for case multiple_54
[56/200] Categorized error for case multiple_55
[57/200] Categorized error for case multiple_56
[58/200] Categorized error for case multiple_57
[59/200] Categorized error for case multiple_58
[60/200] Categorized error for case multiple_59
[61/200] Categorized error for case multiple_60
[62/200] Categorized error for case multiple_61
[63/200] Categorized error for case multiple_62
[64/200] Categorized error for case multiple_63
[65/200] Categorized error for case multiple_64
[66/200] Categorized error for case multiple_65
[67/200] Categorized error for case multiple_66
[68/200] Categorized error for case multiple_67
[69/200] Categorized error for case multiple_68
[70/200] Categorized error for case multiple_69
[71/200] Categorized error for case multiple_70
[72/200] Categorized error for case multiple_71
[73/200] Categorized error for case multiple_72
[74/200] Categorized error for case multiple_73
[75/200] Categorized error for case multiple_74
[76/200] Categorized error for case multiple_75
[77/200] Categorized error for case multiple_76
[78/200] Categorized error for case multiple_77
[79/200] Categorized error for case multiple_78
[80/200] Categorized error for case multiple_79
[81/200] Categorized error for case multiple_80
[82/200] Categorized error for case multiple_81
[83/200] Categorized error for case multiple_82
[84/200] Categorized error for case multiple_83
[85/200] Categorized error for case multiple_84
[86/200] Categorized error for case multiple_85
[87/200] Categorized error for case multiple_86
[88/200] Categorized error for case multiple_87
[89/200] Categorized error for case multiple_88
[90/200] Categorized error for case multiple_89
[91/200] Categorized error for case multiple_90
[92/200] Categorized error for case multiple_91
[93/200] Categorized error for case multiple_92
[94/200] Categorized error for case multiple_93
[95/200] Categorized error for case multiple_94
[96/200] Categorized error for case multiple_95
[97/200] Categorized error for case multiple_96
[98/200] Categorized error for case multiple_97
[99/200] Categorized error for case multiple_98
[100/200] Categorized error for case multiple_99
[101/200] Categorized error for case multiple_100
[102/200] Categorized error for case multiple_101
[103/200] Categorized error for case multiple_102
[104/200] Categorized error for case multiple_103
[105/200] Categorized error for case multiple_104
[106/200] Categorized error for case multiple_105
[107/200] Categorized error for case multiple_106
[108/200] Categorized error for case multiple_107
[109/200] Categorized error for case multiple_108
[110/200] Categorized error for case multiple_109
[111/200] Categorized error for case multiple_110
[112/200] Categorized error for case multiple_111
[113/200] Categorized error for case multiple_112
[114/200] Categorized error for case multiple_113
[115/200] Categorized error for case multiple_114
[116/200] Categorized error for case multiple_115
[117/200] Categorized error for case multiple_116
[118/200] Categorized error for case multiple_117
[119/200] Categorized error for case multiple_118
[120/200] Categorized error for case multiple_119
[121/200] Categorized error for case multiple_120
[122/200] Categorized error for case multiple_121
[123/200] Categorized error for case multiple_122
[124/200] Categorized error for case multiple_123
[125/200] Categorized error for case multiple_124
[126/200] Categorized error for case multiple_125
[127/200] Categorized error for case multiple_126
[128/200] Categorized error for case multiple_127
[129/200] Categorized error for case multiple_128
[130/200] Categorized error for case multiple_129
[131/200] Categorized error for case multiple_130
[132/200] Categorized error for case multiple_131
[133/200] Categorized error for case multiple_132
[134/200] Categorized error for case multiple_133
[135/200] Categorized error for case multiple_134
[136/200] Categorized error for case multiple_135
[137/200] Categorized error for case multiple_136
[138/200] Categorized error for case multiple_137
[139/200] Categorized error for case multiple_138
[140/200] Categorized error for case multiple_139
[141/200] Categorized error for case multiple_140
[142/200] Categorized error for case multiple_141
[143/200] Categorized error for case multiple_142
[144/200] Categorized error for case multiple_143
[145/200] Categorized error for case multiple_144
[146/200] Categorized error for case multiple_145
[147/200] Categorized error for case multiple_146
[148/200] Categorized error for case multiple_147
[149/200] Categorized error for case multiple_148
[150/200] Categorized error for case multiple_149
[151/200] Categorized error for case multiple_150
[152/200] Categorized error for case multiple_151
[153/200] Categorized error for case multiple_152
[154/200] Categorized error for case multiple_153
[155/200] Categorized error for case multiple_154
[156/200] Categorized error for case multiple_155
[157/200] Categorized error for case multiple_156
[158/200] Categorized error for case multiple_157
[159/200] Categorized error for case multiple_158
[160/200] Categorized error for case multiple_159
[161/200] Categorized error for case multiple_160
[162/200] Categorized error for case multiple_161
[163/200] Categorized error for case multiple_162
[164/200] Categorized error for case multiple_163
[165/200] Categorized error for case multiple_164
[166/200] Categorized error for case multiple_165
[167/200] Categorized error for case multiple_166
[168/200] Categorized error for case multiple_167
[169/200] Categorized error for case multiple_168
[170/200] Categorized error for case multiple_169
[171/200] Categorized error for case multiple_170
[172/200] Categorized error for case multiple_171
[173/200] Categorized error for case multiple_172
[174/200] Categorized error for case multiple_173
[175/200] Categorized error for case multiple_174
[176/200] Categorized error for case multiple_175
[177/200] Categorized error for case multiple_176
[178/200] Categorized error for case multiple_177
[179/200] Categorized error for case multiple_178
[180/200] Categorized error for case multiple_179
[181/200] Categorized error for case multiple_180
[182/200] Categorized error for case multiple_181
[183/200] Categorized error for case multiple_182
[184/200] Categorized error for case multiple_183
[185/200] Categorized error for case multiple_184
[186/200] Categorized error for case multiple_185
[187/200] Categorized error for case multiple_186
[188/200] Categorized error for case multiple_187
[189/200] Categorized error for case multiple_188
[190/200] Categorized error for case multiple_189
[191/200] Categorized error for case multiple_190
[192/200] Categorized error for case multiple_191
[193/200] Categorized error for case multiple_192
[194/200] Categorized error for case multiple_193
[195/200] Categorized error for case multiple_194
[196/200] Categorized error for case multiple_195
[197/200] Categorized error for case multiple_196
[198/200] Categorized error for case multiple_197
[199/200] Categorized error for case multiple_198
[200/200] Categorized error for case multiple_199
All 200 error samples categorized.
Released lock for category cache file.
Completed processing for config: ToolConfig { model: Local(meta-llama/Llama-3.1-8B-Instruct), translate_mode: NotTranslated, add_noise_mode: NoNoise }
vLLM backend created successfully for meta-llama/Llama-3.1-8B-Instruct
ERROR 12-16 11:16:21 [core_client.py:600] Engine core proc EngineCore_DP0 died unexpectedly, shutting down client.
