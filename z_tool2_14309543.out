‚úèÔ∏è Setting installed package as editable
For dataset file ["BFCL_v4_multiple","zh","fulltrans","nonoise"].jsonl, total entries: 200, existing entries: 200, missing entries: 0
For dataset file ["BFCL_v4_multiple","zh","fulltrans","syno"].jsonl, total entries: 200, existing entries: 200, missing entries: 0
For dataset file ["BFCL_v4_multiple","zh","fulltrans","para"].jsonl, total entries: 200, existing entries: 200, missing entries: 0
Wrote aggregated questions to file: tool/result/meta-llama-Llama-3.1-70B-Instruct/pre_translate_aggregated_questions_input.jsonl
For result file ["BFCL_v4_multiple","zh","fulltrans","nonoise"].jsonl, target entries: 200, dispatched entries: 200, missing entries: 0
Wrote pre-translate result file: "tool/result/meta-llama-Llama-3.1-70B-Instruct/pre_translate/[\"BFCL_v4_multiple\",\"zh\",\"fulltrans\",\"nonoise\"].jsonl"
For result file ["BFCL_v4_multiple","zh","fulltrans","syno"].jsonl, target entries: 200, dispatched entries: 200, missing entries: 0
Wrote pre-translate result file: "tool/result/meta-llama-Llama-3.1-70B-Instruct/pre_translate/[\"BFCL_v4_multiple\",\"zh\",\"fulltrans\",\"syno\"].jsonl"
For result file ["BFCL_v4_multiple","zh","fulltrans","para"].jsonl, target entries: 200, dispatched entries: 200, missing entries: 0
Wrote pre-translate result file: "tool/result/meta-llama-Llama-3.1-70B-Instruct/pre_translate/[\"BFCL_v4_multiple\",\"zh\",\"fulltrans\",\"para\"].jsonl"
Removed pre-translate aggregated questions output file: tool/result/meta-llama-Llama-3.1-70B-Instruct/pre_translate_aggregated_questions_output.jsonl
For result file tool/result/meta-llama-Llama-3.1-70B-Instruct/generate_raw/["BFCL_v4_multiple","zh","fulltrans","pretrans","syno","noprompt"].jsonl, total entries: 200, existing entries: 200, missing entries to generate: 0
For result file tool/result/meta-llama-Llama-3.1-70B-Instruct/generate_raw/["BFCL_v4_multiple","zh","fulltrans","pretrans","para","noprompt"].jsonl, total entries: 200, existing entries: 200, missing entries to generate: 0
For result file tool/result/meta-llama-Llama-3.1-70B-Instruct/generate_raw/["BFCL_v4_multiple","en","na","nopretrans","nonoise","noprompt"].jsonl, total entries: 200, existing entries: 200, missing entries to generate: 0
For result file tool/result/meta-llama-Llama-3.1-70B-Instruct/generate_raw/["BFCL_v4_multiple","zh","parttrans","nopretrans","para","noprompt"].jsonl, total entries: 200, existing entries: 200, missing entries to generate: 0
For result file tool/result/meta-llama-Llama-3.1-70B-Instruct/generate_raw/["BFCL_v4_multiple","en","na","nopretrans","syno","noprompt"].jsonl, total entries: 200, existing entries: 200, missing entries to generate: 0
For result file tool/result/meta-llama-Llama-3.1-70B-Instruct/generate_raw/["BFCL_v4_multiple","zh","fulltrans","nopretrans","syno","noprompt"].jsonl, total entries: 200, existing entries: 200, missing entries to generate: 0
For result file tool/result/meta-llama-Llama-3.1-70B-Instruct/generate_raw/["BFCL_v4_multiple","zh","parttrans","nopretrans","syno","noprompt"].jsonl, total entries: 200, existing entries: 200, missing entries to generate: 0
For result file tool/result/meta-llama-Llama-3.1-70B-Instruct/generate_raw/["BFCL_v4_multiple","zh","fulltrans","nopretrans","syno","prompt"].jsonl, total entries: 200, existing entries: 200, missing entries to generate: 0
For result file tool/result/meta-llama-Llama-3.1-70B-Instruct/generate_raw/["BFCL_v4_multiple","zh","fulltrans","nopretrans","nonoise","prompt"].jsonl, total entries: 200, existing entries: 200, missing entries to generate: 0
For result file tool/result/meta-llama-Llama-3.1-70B-Instruct/generate_raw/["BFCL_v4_multiple","en","na","nopretrans","para","noprompt"].jsonl, total entries: 200, existing entries: 200, missing entries to generate: 0
For result file tool/result/meta-llama-Llama-3.1-70B-Instruct/generate_raw/["BFCL_v4_multiple","zh","fulltrans","nopretrans","para","prompt"].jsonl, total entries: 200, existing entries: 200, missing entries to generate: 0
For result file tool/result/meta-llama-Llama-3.1-70B-Instruct/generate_raw/["BFCL_v4_multiple","zh","fulltrans","nopretrans","para","noprompt"].jsonl, total entries: 200, existing entries: 200, missing entries to generate: 0
For result file tool/result/meta-llama-Llama-3.1-70B-Instruct/generate_raw/["BFCL_v4_multiple","zh","parttrans","nopretrans","nonoise","noprompt"].jsonl, total entries: 200, existing entries: 200, missing entries to generate: 0
For result file tool/result/meta-llama-Llama-3.1-70B-Instruct/generate_raw/["BFCL_v4_multiple","zh","fulltrans","nopretrans","nonoise","noprompt"].jsonl, total entries: 200, existing entries: 200, missing entries to generate: 0
For result file tool/result/meta-llama-Llama-3.1-70B-Instruct/generate_raw/["BFCL_v4_multiple","zh","fulltrans","pretrans","nonoise","noprompt"].jsonl, total entries: 200, existing entries: 200, missing entries to generate: 0
Wrote aggregated input file to tool/result/meta-llama-Llama-3.1-70B-Instruct/generate_raw_aggregated_input.jsonl
For result file ["BFCL_v4_multiple","en","na","nopretrans","nonoise","noprompt"].jsonl, target entries: 200, dispatched entries: 200, missing entries: 0
Wrote result file to tool/result/meta-llama-Llama-3.1-70B-Instruct/generate_raw/["BFCL_v4_multiple","en","na","nopretrans","nonoise","noprompt"].jsonl
For result file ["BFCL_v4_multiple","zh","fulltrans","nopretrans","nonoise","noprompt"].jsonl, target entries: 200, dispatched entries: 200, missing entries: 0
Wrote result file to tool/result/meta-llama-Llama-3.1-70B-Instruct/generate_raw/["BFCL_v4_multiple","zh","fulltrans","nopretrans","nonoise","noprompt"].jsonl
For result file ["BFCL_v4_multiple","zh","parttrans","nopretrans","nonoise","noprompt"].jsonl, target entries: 200, dispatched entries: 200, missing entries: 0
Wrote result file to tool/result/meta-llama-Llama-3.1-70B-Instruct/generate_raw/["BFCL_v4_multiple","zh","parttrans","nopretrans","nonoise","noprompt"].jsonl
For result file ["BFCL_v4_multiple","zh","fulltrans","nopretrans","nonoise","prompt"].jsonl, target entries: 200, dispatched entries: 200, missing entries: 0
Wrote result file to tool/result/meta-llama-Llama-3.1-70B-Instruct/generate_raw/["BFCL_v4_multiple","zh","fulltrans","nopretrans","nonoise","prompt"].jsonl
For result file ["BFCL_v4_multiple","zh","fulltrans","pretrans","nonoise","noprompt"].jsonl, target entries: 200, dispatched entries: 200, missing entries: 0
Wrote result file to tool/result/meta-llama-Llama-3.1-70B-Instruct/generate_raw/["BFCL_v4_multiple","zh","fulltrans","pretrans","nonoise","noprompt"].jsonl
For result file ["BFCL_v4_multiple","zh","fulltrans","nopretrans","nonoise","noprompt"].jsonl, target entries: 200, dispatched entries: 200, missing entries: 0
Wrote result file to tool/result/meta-llama-Llama-3.1-70B-Instruct/generate_raw/["BFCL_v4_multiple","zh","fulltrans","nopretrans","nonoise","noprompt"].jsonl
For result file ["BFCL_v4_multiple","en","na","nopretrans","syno","noprompt"].jsonl, target entries: 200, dispatched entries: 200, missing entries: 0
Wrote result file to tool/result/meta-llama-Llama-3.1-70B-Instruct/generate_raw/["BFCL_v4_multiple","en","na","nopretrans","syno","noprompt"].jsonl
For result file ["BFCL_v4_multiple","zh","fulltrans","nopretrans","syno","noprompt"].jsonl, target entries: 200, dispatched entries: 200, missing entries: 0
Wrote result file to tool/result/meta-llama-Llama-3.1-70B-Instruct/generate_raw/["BFCL_v4_multiple","zh","fulltrans","nopretrans","syno","noprompt"].jsonl
For result file ["BFCL_v4_multiple","zh","parttrans","nopretrans","syno","noprompt"].jsonl, target entries: 200, dispatched entries: 200, missing entries: 0
Wrote result file to tool/result/meta-llama-Llama-3.1-70B-Instruct/generate_raw/["BFCL_v4_multiple","zh","parttrans","nopretrans","syno","noprompt"].jsonl
For result file ["BFCL_v4_multiple","zh","fulltrans","nopretrans","syno","prompt"].jsonl, target entries: 200, dispatched entries: 200, missing entries: 0
Wrote result file to tool/result/meta-llama-Llama-3.1-70B-Instruct/generate_raw/["BFCL_v4_multiple","zh","fulltrans","nopretrans","syno","prompt"].jsonl
For result file ["BFCL_v4_multiple","zh","fulltrans","pretrans","syno","noprompt"].jsonl, target entries: 200, dispatched entries: 200, missing entries: 0
Wrote result file to tool/result/meta-llama-Llama-3.1-70B-Instruct/generate_raw/["BFCL_v4_multiple","zh","fulltrans","pretrans","syno","noprompt"].jsonl
For result file ["BFCL_v4_multiple","zh","fulltrans","nopretrans","syno","noprompt"].jsonl, target entries: 200, dispatched entries: 200, missing entries: 0
Wrote result file to tool/result/meta-llama-Llama-3.1-70B-Instruct/generate_raw/["BFCL_v4_multiple","zh","fulltrans","nopretrans","syno","noprompt"].jsonl
For result file ["BFCL_v4_multiple","en","na","nopretrans","para","noprompt"].jsonl, target entries: 200, dispatched entries: 200, missing entries: 0
Wrote result file to tool/result/meta-llama-Llama-3.1-70B-Instruct/generate_raw/["BFCL_v4_multiple","en","na","nopretrans","para","noprompt"].jsonl
For result file ["BFCL_v4_multiple","zh","fulltrans","nopretrans","para","noprompt"].jsonl, target entries: 200, dispatched entries: 200, missing entries: 0
Wrote result file to tool/result/meta-llama-Llama-3.1-70B-Instruct/generate_raw/["BFCL_v4_multiple","zh","fulltrans","nopretrans","para","noprompt"].jsonl
For result file ["BFCL_v4_multiple","zh","parttrans","nopretrans","para","noprompt"].jsonl, target entries: 200, dispatched entries: 200, missing entries: 0
Wrote result file to tool/result/meta-llama-Llama-3.1-70B-Instruct/generate_raw/["BFCL_v4_multiple","zh","parttrans","nopretrans","para","noprompt"].jsonl
For result file ["BFCL_v4_multiple","zh","fulltrans","nopretrans","para","prompt"].jsonl, target entries: 200, dispatched entries: 200, missing entries: 0
Wrote result file to tool/result/meta-llama-Llama-3.1-70B-Instruct/generate_raw/["BFCL_v4_multiple","zh","fulltrans","nopretrans","para","prompt"].jsonl
For result file ["BFCL_v4_multiple","zh","fulltrans","pretrans","para","noprompt"].jsonl, target entries: 200, dispatched entries: 200, missing entries: 0
Wrote result file to tool/result/meta-llama-Llama-3.1-70B-Instruct/generate_raw/["BFCL_v4_multiple","zh","fulltrans","pretrans","para","noprompt"].jsonl
For result file ["BFCL_v4_multiple","zh","fulltrans","nopretrans","para","noprompt"].jsonl, target entries: 200, dispatched entries: 200, missing entries: 0
Wrote result file to tool/result/meta-llama-Llama-3.1-70B-Instruct/generate_raw/["BFCL_v4_multiple","zh","fulltrans","nopretrans","para","noprompt"].jsonl
Removed aggregated output file tool/result/meta-llama-Llama-3.1-70B-Instruct/generate_raw_aggregated_output.jsonl
Pass parse output completed for model meta-llama-Llama-3.1-70B-Instruct
Post translate result file not found, assuming no existing entries.
Post translate: for result file GenerateRawFileName(BfclV4Multiple, Chinese, FullyTranslated, NoPreTranslate, NoNoise, NoPromptTranslate), found 200 total entries, 0 existing entries, 200 missing entries.
Post translate result file not found, assuming no existing entries.
Post translate: for result file GenerateRawFileName(BfclV4Multiple, Chinese, FullyTranslated, NoPreTranslate, Synonym, NoPromptTranslate), found 200 total entries, 0 existing entries, 200 missing entries.
Post translate result file not found, assuming no existing entries.
Post translate: for result file GenerateRawFileName(BfclV4Multiple, Chinese, FullyTranslated, NoPreTranslate, Paraphrase, NoPromptTranslate), found 200 total entries, 0 existing entries, 200 missing entries.
Pass post translate prepare aggregated input completed for model meta-llama-Llama-3.1-70B-Instruct
Acquiring build lock...
Building Rust extension with maturin develop...
Installed Rust extension successfully.
Released build lock.
Loading config from: tool_config_slurm2.py
Processing configuration:  <builtins.ToolConfig object at 0x7fbf760c36b0>
----------PASS 1: PRE-TRANSLATE QUESTIONS----------
----------PASS 2: GENERATE RAW FUNCTION CALLS----------
----------PASS 3: PARSE FUNCTION CALLS----------
----------PASS 4: POST-TRANSLATE FUNCTION CALLS----------
Creating vLLM backend for model LocalModel.Llama3_1_70B...
Creating vLLM backend for model meta-llama/Llama-3.1-70B-Instruct with 8 GPUs...
INFO 12-22 20:45:35 [model.py:637] Resolved architecture: LlamaForCausalLM
INFO 12-22 20:45:35 [model.py:1750] Using max model len 5000
INFO 12-22 20:45:38 [scheduler.py:228] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(EngineCore_DP0 pid=1942462)[0;0m INFO 12-22 20:45:45 [core.py:93] Initializing a V1 LLM engine (v0.12.0) with config: model='meta-llama/Llama-3.1-70B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-70B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=5000, download_dir=None, load_format=auto, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01), seed=0, served_model_name=meta-llama/Llama-3.1-70B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 256, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=1942462)[0;0m WARNING 12-22 20:45:45 [multiproc_executor.py:880] Reducing Torch parallelism from 4 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[0;36m(EngineCore_DP0 pid=1942462)[0;0m INFO 12-22 20:46:09 [parallel_state.py:1200] world_size=8 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:34979 backend=nccl
[0;36m(EngineCore_DP0 pid=1942462)[0;0m INFO 12-22 20:46:09 [parallel_state.py:1200] world_size=8 rank=6 local_rank=6 distributed_init_method=tcp://127.0.0.1:34979 backend=nccl
[0;36m(EngineCore_DP0 pid=1942462)[0;0m INFO 12-22 20:46:09 [parallel_state.py:1200] world_size=8 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:34979 backend=nccl
[0;36m(EngineCore_DP0 pid=1942462)[0;0m INFO 12-22 20:46:09 [parallel_state.py:1200] world_size=8 rank=7 local_rank=7 distributed_init_method=tcp://127.0.0.1:34979 backend=nccl
[0;36m(EngineCore_DP0 pid=1942462)[0;0m INFO 12-22 20:46:09 [parallel_state.py:1200] world_size=8 rank=5 local_rank=5 distributed_init_method=tcp://127.0.0.1:34979 backend=nccl
[0;36m(EngineCore_DP0 pid=1942462)[0;0m INFO 12-22 20:46:09 [parallel_state.py:1200] world_size=8 rank=2 local_rank=2 distributed_init_method=tcp://127.0.0.1:34979 backend=nccl
[0;36m(EngineCore_DP0 pid=1942462)[0;0m INFO 12-22 20:46:09 [parallel_state.py:1200] world_size=8 rank=4 local_rank=4 distributed_init_method=tcp://127.0.0.1:34979 backend=nccl
[0;36m(EngineCore_DP0 pid=1942462)[0;0m INFO 12-22 20:46:09 [parallel_state.py:1200] world_size=8 rank=3 local_rank=3 distributed_init_method=tcp://127.0.0.1:34979 backend=nccl
[0;36m(EngineCore_DP0 pid=1942462)[0;0m INFO 12-22 20:46:11 [pynccl.py:111] vLLM is using nccl==2.27.5
[0;36m(EngineCore_DP0 pid=1942462)[0;0m INFO 12-22 20:46:17 [parallel_state.py:1408] rank 2 in world size 8 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 2, EP rank 2
[0;36m(EngineCore_DP0 pid=1942462)[0;0m INFO 12-22 20:46:17 [parallel_state.py:1408] rank 5 in world size 8 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 5, EP rank 5
[0;36m(EngineCore_DP0 pid=1942462)[0;0m INFO 12-22 20:46:17 [parallel_state.py:1408] rank 1 in world size 8 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
[0;36m(EngineCore_DP0 pid=1942462)[0;0m INFO 12-22 20:46:17 [parallel_state.py:1408] rank 0 in world size 8 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=1942462)[0;0m INFO 12-22 20:46:17 [parallel_state.py:1408] rank 4 in world size 8 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 4, EP rank 4
[0;36m(EngineCore_DP0 pid=1942462)[0;0m INFO 12-22 20:46:17 [parallel_state.py:1408] rank 3 in world size 8 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 3, EP rank 3
[0;36m(EngineCore_DP0 pid=1942462)[0;0m INFO 12-22 20:46:17 [parallel_state.py:1408] rank 7 in world size 8 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 7, EP rank 7
[0;36m(EngineCore_DP0 pid=1942462)[0;0m INFO 12-22 20:46:17 [parallel_state.py:1408] rank 6 in world size 8 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 6, EP rank 6
[0;36m(EngineCore_DP0 pid=1942462)[0;0m [0;36m(Worker_TP0 pid=1942469)[0;0m INFO 12-22 20:46:18 [gpu_model_runner.py:3467] Starting to load model meta-llama/Llama-3.1-70B-Instruct...
[0;36m(EngineCore_DP0 pid=1942462)[0;0m [0;36m(Worker_TP3 pid=1942475)[0;0m INFO 12-22 20:46:22 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=1942462)[0;0m [0;36m(Worker_TP5 pid=1942479)[0;0m INFO 12-22 20:46:22 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=1942462)[0;0m [0;36m(Worker_TP1 pid=1942471)[0;0m INFO 12-22 20:46:22 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=1942462)[0;0m [0;36m(Worker_TP2 pid=1942473)[0;0m INFO 12-22 20:46:22 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=1942462)[0;0m [0;36m(Worker_TP0 pid=1942469)[0;0m INFO 12-22 20:46:22 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=1942462)[0;0m [0;36m(Worker_TP4 pid=1942477)[0;0m INFO 12-22 20:46:22 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=1942462)[0;0m [0;36m(Worker_TP7 pid=1942483)[0;0m INFO 12-22 20:46:22 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=1942462)[0;0m [0;36m(Worker_TP6 pid=1942481)[0;0m INFO 12-22 20:46:22 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=1942462)[0;0m [0;36m(Worker_TP0 pid=1942469)[0;0m INFO 12-22 20:48:09 [default_loader.py:308] Loading weights took 105.01 seconds
[0;36m(EngineCore_DP0 pid=1942462)[0;0m [0;36m(Worker_TP0 pid=1942469)[0;0m INFO 12-22 20:48:10 [gpu_model_runner.py:3549] Model loading took 16.4607 GiB memory and 111.103497 seconds
[0;36m(EngineCore_DP0 pid=1942462)[0;0m [0;36m(Worker_TP0 pid=1942469)[0;0m INFO 12-22 20:48:43 [backends.py:655] Using cache directory: /u/zluo8/.cache/vllm/torch_compile_cache/b664ddeda6/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=1942462)[0;0m [0;36m(Worker_TP0 pid=1942469)[0;0m INFO 12-22 20:48:43 [backends.py:715] Dynamo bytecode transform time: 31.47 s
[0;36m(EngineCore_DP0 pid=1942462)[0;0m [0;36m(Worker_TP3 pid=1942475)[0;0m INFO 12-22 20:48:54 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 9.603 s
[0;36m(EngineCore_DP0 pid=1942462)[0;0m [0;36m(Worker_TP6 pid=1942481)[0;0m INFO 12-22 20:48:55 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 10.577 s
[0;36m(EngineCore_DP0 pid=1942462)[0;0m [0;36m(Worker_TP0 pid=1942469)[0;0m INFO 12-22 20:48:55 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 10.428 s
[0;36m(EngineCore_DP0 pid=1942462)[0;0m [0;36m(Worker_TP7 pid=1942483)[0;0m INFO 12-22 20:48:55 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 11.466 s
[0;36m(EngineCore_DP0 pid=1942462)[0;0m [0;36m(Worker_TP1 pid=1942471)[0;0m INFO 12-22 20:48:55 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 10.660 s
[0;36m(EngineCore_DP0 pid=1942462)[0;0m [0;36m(Worker_TP4 pid=1942477)[0;0m INFO 12-22 20:48:55 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 10.909 s
[0;36m(EngineCore_DP0 pid=1942462)[0;0m [0;36m(Worker_TP2 pid=1942473)[0;0m INFO 12-22 20:48:56 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 11.697 s
[0;36m(EngineCore_DP0 pid=1942462)[0;0m [0;36m(Worker_TP5 pid=1942479)[0;0m INFO 12-22 20:48:56 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 11.876 s
[0;36m(EngineCore_DP0 pid=1942462)[0;0m [0;36m(Worker_TP0 pid=1942469)[0;0m INFO 12-22 20:49:07 [monitor.py:34] torch.compile takes 41.90 s in total
[0;36m(EngineCore_DP0 pid=1942462)[0;0m INFO 12-22 20:49:10 [shm_broadcast.py:501] No available shared memory broadcast block found in 60 seconds. This typically happens when some processes are hanging or doing some time-consuming work (e.g. compilation, weight/kv cache quantization).
[0;36m(EngineCore_DP0 pid=1942462)[0;0m [0;36m(Worker_TP0 pid=1942469)[0;0m INFO 12-22 20:49:12 [gpu_worker.py:359] Available KV cache memory: 108.06 GiB
[0;36m(EngineCore_DP0 pid=1942462)[0;0m INFO 12-22 20:49:14 [kv_cache_utils.py:1286] GPU KV cache size: 2,832,768 tokens
[0;36m(EngineCore_DP0 pid=1942462)[0;0m INFO 12-22 20:49:14 [kv_cache_utils.py:1291] Maximum concurrency for 5,000 tokens per request: 565.65x
[0;36m(EngineCore_DP0 pid=1942462)[0;0m [0;36m(Worker_TP5 pid=1942479)[0;0m INFO 12-22 20:49:32 [custom_all_reduce.py:216] Registering 1288 cuda graph addresses
[0;36m(EngineCore_DP0 pid=1942462)[0;0m [0;36m(Worker_TP0 pid=1942469)[0;0m INFO 12-22 20:49:32 [custom_all_reduce.py:216] Registering 1288 cuda graph addresses
[0;36m(EngineCore_DP0 pid=1942462)[0;0m [0;36m(Worker_TP6 pid=1942481)[0;0m INFO 12-22 20:49:32 [custom_all_reduce.py:216] Registering 1288 cuda graph addresses
[0;36m(EngineCore_DP0 pid=1942462)[0;0m [0;36m(Worker_TP2 pid=1942473)[0;0m INFO 12-22 20:49:32 [custom_all_reduce.py:216] Registering 1288 cuda graph addresses
[0;36m(EngineCore_DP0 pid=1942462)[0;0m [0;36m(Worker_TP4 pid=1942477)[0;0m INFO 12-22 20:49:32 [custom_all_reduce.py:216] Registering 1288 cuda graph addresses
[0;36m(EngineCore_DP0 pid=1942462)[0;0m [0;36m(Worker_TP1 pid=1942471)[0;0m INFO 12-22 20:49:32 [custom_all_reduce.py:216] Registering 1288 cuda graph addresses
[0;36m(EngineCore_DP0 pid=1942462)[0;0m [0;36m(Worker_TP7 pid=1942483)[0;0m INFO 12-22 20:49:32 [custom_all_reduce.py:216] Registering 1288 cuda graph addresses
[0;36m(EngineCore_DP0 pid=1942462)[0;0m [0;36m(Worker_TP3 pid=1942475)[0;0m INFO 12-22 20:49:32 [custom_all_reduce.py:216] Registering 1288 cuda graph addresses
[0;36m(EngineCore_DP0 pid=1942462)[0;0m [0;36m(Worker_TP0 pid=1942469)[0;0m INFO 12-22 20:49:33 [gpu_model_runner.py:4466] Graph capturing finished in 19 secs, took 0.88 GiB
[0;36m(EngineCore_DP0 pid=1942462)[0;0m INFO 12-22 20:49:34 [core.py:254] init engine (profile, create kv cache, warmup model) took 83.10 seconds
Post translate result file not found, starting fresh.
Post translate: for parse output file GenerateRawFileName(BfclV4Multiple, Chinese, FullyTranslated, NoPreTranslate, NoNoise, NoPromptTranslate), translated 200 missing entries, 0 entries failed to translate.
Pass post translate dispatch results completed for model meta-llama-Llama-3.1-70B-Instruct, file GenerateRawFileName(BfclV4Multiple, Chinese, FullyTranslated, NoPreTranslate, NoNoise, NoPromptTranslate)
Post translate result file not found, starting fresh.
Post translate: for parse output file GenerateRawFileName(BfclV4Multiple, Chinese, FullyTranslated, NoPreTranslate, Synonym, NoPromptTranslate), translated 200 missing entries, 0 entries failed to translate.
Pass post translate dispatch results completed for model meta-llama-Llama-3.1-70B-Instruct, file GenerateRawFileName(BfclV4Multiple, Chinese, FullyTranslated, NoPreTranslate, Synonym, NoPromptTranslate)
Post translate result file not found, starting fresh.
Post translate: for parse output file GenerateRawFileName(BfclV4Multiple, Chinese, FullyTranslated, NoPreTranslate, Paraphrase, NoPromptTranslate), translated 200 missing entries, 0 entries failed to translate.
Pass post translate dispatch results completed for model meta-llama-Llama-3.1-70B-Instruct, file GenerateRawFileName(BfclV4Multiple, Chinese, FullyTranslated, NoPreTranslate, Paraphrase, NoPromptTranslate)
Removed temporary aggregated output file "tool/result/meta-llama-Llama-3.1-70B-Instruct/post_translate_aggregated_output.jsonl"
Wrote evaluate entries to file: "tool/result/meta-llama-Llama-3.1-70B-Instruct/evaluate/[\"BFCL_v4_multiple\",\"en\",\"na\",\"nopretrans\",\"nonoise\",\"noprompt\",\"noposttrans\"].jsonl"
Wrote evaluate entries to file: "tool/result/meta-llama-Llama-3.1-70B-Instruct/evaluate/[\"BFCL_v4_multiple\",\"zh\",\"fulltrans\",\"nopretrans\",\"nonoise\",\"noprompt\",\"noposttrans\"].jsonl"
Wrote evaluate entries to file: "tool/result/meta-llama-Llama-3.1-70B-Instruct/evaluate/[\"BFCL_v4_multiple\",\"zh\",\"parttrans\",\"nopretrans\",\"nonoise\",\"noprompt\",\"noposttrans\"].jsonl"
Wrote evaluate entries to file: "tool/result/meta-llama-Llama-3.1-70B-Instruct/evaluate/[\"BFCL_v4_multiple\",\"zh\",\"fulltrans\",\"nopretrans\",\"nonoise\",\"prompt\",\"noposttrans\"].jsonl"
Wrote evaluate entries to file: "tool/result/meta-llama-Llama-3.1-70B-Instruct/evaluate/[\"BFCL_v4_multiple\",\"zh\",\"fulltrans\",\"pretrans\",\"nonoise\",\"noprompt\",\"noposttrans\"].jsonl"
Wrote evaluate entries to file: "tool/result/meta-llama-Llama-3.1-70B-Instruct/evaluate/[\"BFCL_v4_multiple\",\"zh\",\"fulltrans\",\"nopretrans\",\"nonoise\",\"noprompt\",\"posttrans\"].jsonl"
Wrote evaluate entries to file: "tool/result/meta-llama-Llama-3.1-70B-Instruct/evaluate/[\"BFCL_v4_multiple\",\"en\",\"na\",\"nopretrans\",\"syno\",\"noprompt\",\"noposttrans\"].jsonl"
Wrote evaluate entries to file: "tool/result/meta-llama-Llama-3.1-70B-Instruct/evaluate/[\"BFCL_v4_multiple\",\"zh\",\"fulltrans\",\"nopretrans\",\"syno\",\"noprompt\",\"noposttrans\"].jsonl"
Wrote evaluate entries to file: "tool/result/meta-llama-Llama-3.1-70B-Instruct/evaluate/[\"BFCL_v4_multiple\",\"zh\",\"parttrans\",\"nopretrans\",\"syno\",\"noprompt\",\"noposttrans\"].jsonl"
Wrote evaluate entries to file: "tool/result/meta-llama-Llama-3.1-70B-Instruct/evaluate/[\"BFCL_v4_multiple\",\"zh\",\"fulltrans\",\"nopretrans\",\"syno\",\"prompt\",\"noposttrans\"].jsonl"
Wrote evaluate entries to file: "tool/result/meta-llama-Llama-3.1-70B-Instruct/evaluate/[\"BFCL_v4_multiple\",\"zh\",\"fulltrans\",\"pretrans\",\"syno\",\"noprompt\",\"noposttrans\"].jsonl"
Wrote evaluate entries to file: "tool/result/meta-llama-Llama-3.1-70B-Instruct/evaluate/[\"BFCL_v4_multiple\",\"zh\",\"fulltrans\",\"nopretrans\",\"syno\",\"noprompt\",\"posttrans\"].jsonl"
Wrote evaluate entries to file: "tool/result/meta-llama-Llama-3.1-70B-Instruct/evaluate/[\"BFCL_v4_multiple\",\"en\",\"na\",\"nopretrans\",\"para\",\"noprompt\",\"noposttrans\"].jsonl"
Wrote evaluate entries to file: "tool/result/meta-llama-Llama-3.1-70B-Instruct/evaluate/[\"BFCL_v4_multiple\",\"zh\",\"fulltrans\",\"nopretrans\",\"para\",\"noprompt\",\"noposttrans\"].jsonl"
Wrote evaluate entries to file: "tool/result/meta-llama-Llama-3.1-70B-Instruct/evaluate/[\"BFCL_v4_multiple\",\"zh\",\"parttrans\",\"nopretrans\",\"para\",\"noprompt\",\"noposttrans\"].jsonl"
Wrote evaluate entries to file: "tool/result/meta-llama-Llama-3.1-70B-Instruct/evaluate/[\"BFCL_v4_multiple\",\"zh\",\"fulltrans\",\"nopretrans\",\"para\",\"prompt\",\"noposttrans\"].jsonl"
Wrote evaluate entries to file: "tool/result/meta-llama-Llama-3.1-70B-Instruct/evaluate/[\"BFCL_v4_multiple\",\"zh\",\"fulltrans\",\"pretrans\",\"para\",\"noprompt\",\"noposttrans\"].jsonl"
Wrote evaluate entries to file: "tool/result/meta-llama-Llama-3.1-70B-Instruct/evaluate/[\"BFCL_v4_multiple\",\"zh\",\"fulltrans\",\"nopretrans\",\"para\",\"noprompt\",\"posttrans\"].jsonl"
Pass evaluate completed for model meta-llama-Llama-3.1-70B-Instruct
Acquiring lock for category cache file...
Acquired lock for category cache file.
Loading category cache from tool/result/category_cache.jsonl...
Releasing lock for category cache file...
Released lock for category cache file.
Wrote categorize aggregated input file at tool/result/meta-llama-Llama-3.1-70B-Instruct/categorize_aggregated_input.jsonl
Acquiring lock for category cache file...
Acquired lock for category cache file.
Loading category cache from tool/result/category_cache.jsonl...
For categorize file EvaluateFileName(BfclV4Multiple, English, NotApplicable, NoPreTranslate, NoNoise, NoPromptTranslate, NoPostTranslate), total ids: 200, missing ids after dispatch: 0
Cache hits: 15, Cache misses: 13
Wrote categorize file after dispatch at "tool/result/meta-llama-Llama-3.1-70B-Instruct/categorize/[\"BFCL_v4_multiple\",\"en\",\"na\",\"nopretrans\",\"nonoise\",\"noprompt\",\"noposttrans\"].jsonl"
For categorize file EvaluateFileName(BfclV4Multiple, Chinese, FullyTranslated, NoPreTranslate, NoNoise, NoPromptTranslate, NoPostTranslate), total ids: 200, missing ids after dispatch: 0
Cache hits: 89, Cache misses: 4
Wrote categorize file after dispatch at "tool/result/meta-llama-Llama-3.1-70B-Instruct/categorize/[\"BFCL_v4_multiple\",\"zh\",\"fulltrans\",\"nopretrans\",\"nonoise\",\"noprompt\",\"noposttrans\"].jsonl"
For categorize file EvaluateFileName(BfclV4Multiple, Chinese, PartiallyTranslated, NoPreTranslate, NoNoise, NoPromptTranslate, NoPostTranslate), total ids: 200, missing ids after dispatch: 0
Cache hits: 31, Cache misses: 0
Wrote categorize file after dispatch at "tool/result/meta-llama-Llama-3.1-70B-Instruct/categorize/[\"BFCL_v4_multiple\",\"zh\",\"parttrans\",\"nopretrans\",\"nonoise\",\"noprompt\",\"noposttrans\"].jsonl"
For categorize file EvaluateFileName(BfclV4Multiple, Chinese, FullyTranslated, NoPreTranslate, NoNoise, PromptTranslate, NoPostTranslate), total ids: 200, missing ids after dispatch: 0
Cache hits: 57, Cache misses: 0
Wrote categorize file after dispatch at "tool/result/meta-llama-Llama-3.1-70B-Instruct/categorize/[\"BFCL_v4_multiple\",\"zh\",\"fulltrans\",\"nopretrans\",\"nonoise\",\"prompt\",\"noposttrans\"].jsonl"
For categorize file EvaluateFileName(BfclV4Multiple, Chinese, FullyTranslated, PreTranslate, NoNoise, NoPromptTranslate, NoPostTranslate), total ids: 200, missing ids after dispatch: 0
Cache hits: 56, Cache misses: 2
Wrote categorize file after dispatch at "tool/result/meta-llama-Llama-3.1-70B-Instruct/categorize/[\"BFCL_v4_multiple\",\"zh\",\"fulltrans\",\"pretrans\",\"nonoise\",\"noprompt\",\"noposttrans\"].jsonl"
For categorize file EvaluateFileName(BfclV4Multiple, Chinese, FullyTranslated, NoPreTranslate, NoNoise, NoPromptTranslate, PostTranslate), total ids: 200, missing ids after dispatch: 0
Cache hits: 77, Cache misses: 3
Wrote categorize file after dispatch at "tool/result/meta-llama-Llama-3.1-70B-Instruct/categorize/[\"BFCL_v4_multiple\",\"zh\",\"fulltrans\",\"nopretrans\",\"nonoise\",\"noprompt\",\"posttrans\"].jsonl"
For categorize file EvaluateFileName(BfclV4Multiple, English, NotApplicable, NoPreTranslate, Synonym, NoPromptTranslate, NoPostTranslate), total ids: 200, missing ids after dispatch: 0
Cache hits: 36, Cache misses: 1
Wrote categorize file after dispatch at "tool/result/meta-llama-Llama-3.1-70B-Instruct/categorize/[\"BFCL_v4_multiple\",\"en\",\"na\",\"nopretrans\",\"syno\",\"noprompt\",\"noposttrans\"].jsonl"
For categorize file EvaluateFileName(BfclV4Multiple, Chinese, FullyTranslated, NoPreTranslate, Synonym, NoPromptTranslate, NoPostTranslate), total ids: 200, missing ids after dispatch: 0
Cache hits: 99, Cache misses: 1
Wrote categorize file after dispatch at "tool/result/meta-llama-Llama-3.1-70B-Instruct/categorize/[\"BFCL_v4_multiple\",\"zh\",\"fulltrans\",\"nopretrans\",\"syno\",\"noprompt\",\"noposttrans\"].jsonl"
For categorize file EvaluateFileName(BfclV4Multiple, Chinese, PartiallyTranslated, NoPreTranslate, Synonym, NoPromptTranslate, NoPostTranslate), total ids: 200, missing ids after dispatch: 0
Cache hits: 41, Cache misses: 1
Wrote categorize file after dispatch at "tool/result/meta-llama-Llama-3.1-70B-Instruct/categorize/[\"BFCL_v4_multiple\",\"zh\",\"parttrans\",\"nopretrans\",\"syno\",\"noprompt\",\"noposttrans\"].jsonl"
For categorize file EvaluateFileName(BfclV4Multiple, Chinese, FullyTranslated, NoPreTranslate, Synonym, PromptTranslate, NoPostTranslate), total ids: 200, missing ids after dispatch: 0
Cache hits: 63, Cache misses: 0
Wrote categorize file after dispatch at "tool/result/meta-llama-Llama-3.1-70B-Instruct/categorize/[\"BFCL_v4_multiple\",\"zh\",\"fulltrans\",\"nopretrans\",\"syno\",\"prompt\",\"noposttrans\"].jsonl"
For categorize file EvaluateFileName(BfclV4Multiple, Chinese, FullyTranslated, PreTranslate, Synonym, NoPromptTranslate, NoPostTranslate), total ids: 200, missing ids after dispatch: 0
Cache hits: 56, Cache misses: 0
Wrote categorize file after dispatch at "tool/result/meta-llama-Llama-3.1-70B-Instruct/categorize/[\"BFCL_v4_multiple\",\"zh\",\"fulltrans\",\"pretrans\",\"syno\",\"noprompt\",\"noposttrans\"].jsonl"
For categorize file EvaluateFileName(BfclV4Multiple, Chinese, FullyTranslated, NoPreTranslate, Synonym, NoPromptTranslate, PostTranslate), total ids: 200, missing ids after dispatch: 0
Cache hits: 86, Cache misses: 2
Wrote categorize file after dispatch at "tool/result/meta-llama-Llama-3.1-70B-Instruct/categorize/[\"BFCL_v4_multiple\",\"zh\",\"fulltrans\",\"nopretrans\",\"syno\",\"noprompt\",\"posttrans\"].jsonl"
For categorize file EvaluateFileName(BfclV4Multiple, English, NotApplicable, NoPreTranslate, Paraphrase, NoPromptTranslate, NoPostTranslate), total ids: 200, missing ids after dispatch: 0
Cache hits: 37, Cache misses: 1
Wrote categorize file after dispatch at "tool/result/meta-llama-Llama-3.1-70B-Instruct/categorize/[\"BFCL_v4_multiple\",\"en\",\"na\",\"nopretrans\",\"para\",\"noprompt\",\"noposttrans\"].jsonl"
For categorize file EvaluateFileName(BfclV4Multiple, Chinese, FullyTranslated, NoPreTranslate, Paraphrase, NoPromptTranslate, NoPostTranslate), total ids: 200, missing ids after dispatch: 0
Cache hits: 96, Cache misses: 0
Wrote categorize file after dispatch at "tool/result/meta-llama-Llama-3.1-70B-Instruct/categorize/[\"BFCL_v4_multiple\",\"zh\",\"fulltrans\",\"nopretrans\",\"para\",\"noprompt\",\"noposttrans\"].jsonl"
For categorize file EvaluateFileName(BfclV4Multiple, Chinese, PartiallyTranslated, NoPreTranslate, Paraphrase, NoPromptTranslate, NoPostTranslate), total ids: 200, missing ids after dispatch: 0
Cache hits: 40, Cache misses: 1
Wrote categorize file after dispatch at "tool/result/meta-llama-Llama-3.1-70B-Instruct/categorize/[\"BFCL_v4_multiple\",\"zh\",\"parttrans\",\"nopretrans\",\"para\",\"noprompt\",\"noposttrans\"].jsonl"
For categorize file EvaluateFileName(BfclV4Multiple, Chinese, FullyTranslated, NoPreTranslate, Paraphrase, PromptTranslate, NoPostTranslate), total ids: 200, missing ids after dispatch: 0
Cache hits: 63, Cache misses: 0
Wrote categorize file after dispatch at "tool/result/meta-llama-Llama-3.1-70B-Instruct/categorize/[\"BFCL_v4_multiple\",\"zh\",\"fulltrans\",\"nopretrans\",\"para\",\"prompt\",\"noposttrans\"].jsonl"
For categorize file EvaluateFileName(BfclV4Multiple, Chinese, FullyTranslated, PreTranslate, Paraphrase, NoPromptTranslate, NoPostTranslate), total ids: 200, missing ids after dispatch: 0
Cache hits: 59, Cache misses: 2
Wrote categorize file after dispatch at "tool/result/meta-llama-Llama-3.1-70B-Instruct/categorize/[\"BFCL_v4_multiple\",\"zh\",\"fulltrans\",\"pretrans\",\"para\",\"noprompt\",\"noposttrans\"].jsonl"
For categorize file EvaluateFileName(BfclV4Multiple, Chinese, FullyTranslated, NoPreTranslate, Paraphrase, NoPromptTranslate, PostTranslate), total ids: 200, missing ids after dispatch: 0
Cache hits: 88, Cache misses: 1
Wrote categorize file after dispatch at "tool/result/meta-llama-Llama-3.1-70B-Instruct/categorize/[\"BFCL_v4_multiple\",\"zh\",\"fulltrans\",\"nopretrans\",\"para\",\"noprompt\",\"posttrans\"].jsonl"
Releasing lock for category cache file...
Released lock for category cache file.
Removed categorize aggregated output file at tool/result/meta-llama-Llama-3.1-70B-Instruct/categorize_aggregated_output.jsonl after dispatch
Wrote statistics to tool/result/meta-llama-Llama-3.1-70B-Instruct/statistics/["BFCL_v4_multiple","en","na","nopretrans","nonoise","noprompt","noposttrans"].json
Wrote statistics to tool/result/meta-llama-Llama-3.1-70B-Instruct/statistics/["BFCL_v4_multiple","zh","fulltrans","nopretrans","nonoise","noprompt","noposttrans"].json
Wrote statistics to tool/result/meta-llama-Llama-3.1-70B-Instruct/statistics/["BFCL_v4_multiple","zh","parttrans","nopretrans","nonoise","noprompt","noposttrans"].json
Wrote statistics to tool/result/meta-llama-Llama-3.1-70B-Instruct/statistics/["BFCL_v4_multiple","zh","fulltrans","nopretrans","nonoise","prompt","noposttrans"].json
Wrote statistics to tool/result/meta-llama-Llama-3.1-70B-Instruct/statistics/["BFCL_v4_multiple","zh","fulltrans","pretrans","nonoise","noprompt","noposttrans"].json
Wrote statistics to tool/result/meta-llama-Llama-3.1-70B-Instruct/statistics/["BFCL_v4_multiple","zh","fulltrans","nopretrans","nonoise","noprompt","posttrans"].json
Wrote statistics to tool/result/meta-llama-Llama-3.1-70B-Instruct/statistics/["BFCL_v4_multiple","en","na","nopretrans","syno","noprompt","noposttrans"].json
Wrote statistics to tool/result/meta-llama-Llama-3.1-70B-Instruct/statistics/["BFCL_v4_multiple","zh","fulltrans","nopretrans","syno","noprompt","noposttrans"].json
Wrote statistics to tool/result/meta-llama-Llama-3.1-70B-Instruct/statistics/["BFCL_v4_multiple","zh","parttrans","nopretrans","syno","noprompt","noposttrans"].json
Wrote statistics to tool/result/meta-llama-Llama-3.1-70B-Instruct/statistics/["BFCL_v4_multiple","zh","fulltrans","nopretrans","syno","prompt","noposttrans"].json
Wrote statistics to tool/result/meta-llama-Llama-3.1-70B-Instruct/statistics/["BFCL_v4_multiple","zh","fulltrans","pretrans","syno","noprompt","noposttrans"].json
Wrote statistics to tool/result/meta-llama-Llama-3.1-70B-Instruct/statistics/["BFCL_v4_multiple","zh","fulltrans","nopretrans","syno","noprompt","posttrans"].json
Wrote statistics to tool/result/meta-llama-Llama-3.1-70B-Instruct/statistics/["BFCL_v4_multiple","en","na","nopretrans","para","noprompt","noposttrans"].json
Wrote statistics to tool/result/meta-llama-Llama-3.1-70B-Instruct/statistics/["BFCL_v4_multiple","zh","fulltrans","nopretrans","para","noprompt","noposttrans"].json
Wrote statistics to tool/result/meta-llama-Llama-3.1-70B-Instruct/statistics/["BFCL_v4_multiple","zh","parttrans","nopretrans","para","noprompt","noposttrans"].json
Wrote statistics to tool/result/meta-llama-Llama-3.1-70B-Instruct/statistics/["BFCL_v4_multiple","zh","fulltrans","nopretrans","para","prompt","noposttrans"].json
Wrote statistics to tool/result/meta-llama-Llama-3.1-70B-Instruct/statistics/["BFCL_v4_multiple","zh","fulltrans","pretrans","para","noprompt","noposttrans"].json
Wrote statistics to tool/result/meta-llama-Llama-3.1-70B-Instruct/statistics/["BFCL_v4_multiple","zh","fulltrans","nopretrans","para","noprompt","posttrans"].json
vLLM backend created successfully for meta-llama/Llama-3.1-70B-Instruct
vLLM backend created successfully for model LocalModel.Llama3_1_70B
Translated 1/374 parameters...
Translated 2/374 parameters...
Translated 3/374 parameters...
Translated 4/374 parameters...
Translated 5/374 parameters...
Translated 6/374 parameters...
Translated 7/374 parameters...
Translated 8/374 parameters...
Translated 9/374 parameters...
Translated 10/374 parameters...
Translated 11/374 parameters...
Translated 12/374 parameters...
Translated 13/374 parameters...
Translated 14/374 parameters...
Translated 15/374 parameters...
Translated 16/374 parameters...
Translated 17/374 parameters...
Translated 18/374 parameters...
Translated 19/374 parameters...
Translated 20/374 parameters...
Translated 21/374 parameters...
Translated 22/374 parameters...
Translated 23/374 parameters...
Translated 24/374 parameters...
Translated 25/374 parameters...
Translated 26/374 parameters...
Translated 27/374 parameters...
Translated 28/374 parameters...
Translated 29/374 parameters...
Translated 30/374 parameters...
Translated 31/374 parameters...
Translated 32/374 parameters...
Translated 33/374 parameters...
Translated 34/374 parameters...
Translated 35/374 parameters...
Translated 36/374 parameters...
Translated 37/374 parameters...
Translated 38/374 parameters...
Translated 39/374 parameters...
Translated 40/374 parameters...
Translated 41/374 parameters...
Translated 42/374 parameters...
Translated 43/374 parameters...
Translated 44/374 parameters...
Translated 45/374 parameters...
Translated 46/374 parameters...
Translated 47/374 parameters...
Translated 48/374 parameters...
Translated 49/374 parameters...
Translated 50/374 parameters...
Translated 51/374 parameters...
Translated 52/374 parameters...
Translated 53/374 parameters...
Translated 54/374 parameters...
Translated 55/374 parameters...
Translated 56/374 parameters...
Translated 57/374 parameters...
Translated 58/374 parameters...
Translated 59/374 parameters...
Translated 60/374 parameters...
Translated 61/374 parameters...
Translated 62/374 parameters...
Translated 63/374 parameters...
Translated 64/374 parameters...
Translated 65/374 parameters...
Translated 66/374 parameters...
Translated 67/374 parameters...
Translated 68/374 parameters...
Translated 69/374 parameters...
Translated 70/374 parameters...
Translated 71/374 parameters...
Translated 72/374 parameters...
Translated 73/374 parameters...
Translated 74/374 parameters...
Translated 75/374 parameters...
Translated 76/374 parameters...
Translated 77/374 parameters...
Translated 78/374 parameters...
Translated 79/374 parameters...
Translated 80/374 parameters...
Translated 81/374 parameters...
Translated 82/374 parameters...
Translated 83/374 parameters...
Translated 84/374 parameters...
Translated 85/374 parameters...
Translated 86/374 parameters...
Translated 87/374 parameters...
Translated 88/374 parameters...
Translated 89/374 parameters...
Translated 90/374 parameters...
Translated 91/374 parameters...
Translated 92/374 parameters...
Translated 93/374 parameters...
Translated 94/374 parameters...
Translated 95/374 parameters...
Translated 96/374 parameters...
Translated 97/374 parameters...
Translated 98/374 parameters...
Translated 99/374 parameters...
Translated 100/374 parameters...
Translated 101/374 parameters...
Translated 102/374 parameters...
Translated 103/374 parameters...
Translated 104/374 parameters...
Translated 105/374 parameters...
Translated 106/374 parameters...
Translated 107/374 parameters...
Translated 108/374 parameters...
Translated 109/374 parameters...
Translated 110/374 parameters...
Translated 111/374 parameters...
Translated 112/374 parameters...
Translated 113/374 parameters...
Translated 114/374 parameters...
Translated 115/374 parameters...
Translated 116/374 parameters...
Translated 117/374 parameters...
Translated 118/374 parameters...
Translated 119/374 parameters...
Translated 120/374 parameters...
Translated 121/374 parameters...
Translated 122/374 parameters...
Translated 123/374 parameters...
Translated 124/374 parameters...
Translated 125/374 parameters...
Translated 126/374 parameters...
Translated 127/374 parameters...
Translated 128/374 parameters...
Translated 129/374 parameters...
Translated 130/374 parameters...
Translated 131/374 parameters...
Translated 132/374 parameters...
Translated 133/374 parameters...
Translated 134/374 parameters...
Translated 135/374 parameters...
Translated 136/374 parameters...
Translated 137/374 parameters...
Translated 138/374 parameters...
Translated 139/374 parameters...
Translated 140/374 parameters...
Translated 141/374 parameters...
Translated 142/374 parameters...
Translated 143/374 parameters...
Translated 144/374 parameters...
Translated 145/374 parameters...
Translated 146/374 parameters...
Translated 147/374 parameters...
Translated 148/374 parameters...
Translated 149/374 parameters...
Translated 150/374 parameters...
Translated 151/374 parameters...
Translated 152/374 parameters...
Translated 153/374 parameters...
Translated 154/374 parameters...
Translated 155/374 parameters...
Translated 156/374 parameters...
Translated 157/374 parameters...
Translated 158/374 parameters...
Translated 159/374 parameters...
Translated 160/374 parameters...
Translated 161/374 parameters...
Translated 162/374 parameters...
Translated 163/374 parameters...
Translated 164/374 parameters...
Translated 165/374 parameters...
Translated 166/374 parameters...
Translated 167/374 parameters...
Translated 168/374 parameters...
Translated 169/374 parameters...
Translated 170/374 parameters...
Translated 171/374 parameters...
Translated 172/374 parameters...
Translated 173/374 parameters...
Translated 174/374 parameters...
Translated 175/374 parameters...
Translated 176/374 parameters...
Translated 177/374 parameters...
Translated 178/374 parameters...
Translated 179/374 parameters...
Translated 180/374 parameters...
Translated 181/374 parameters...
Translated 182/374 parameters...
Translated 183/374 parameters...
Translated 184/374 parameters...
Translated 185/374 parameters...
Translated 186/374 parameters...
Translated 187/374 parameters...
Translated 188/374 parameters...
Translated 189/374 parameters...
Translated 190/374 parameters...
Translated 191/374 parameters...
Translated 192/374 parameters...
Translated 193/374 parameters...
Translated 194/374 parameters...
Translated 195/374 parameters...
Translated 196/374 parameters...
Translated 197/374 parameters...
Translated 198/374 parameters...
Translated 199/374 parameters...
Translated 200/374 parameters...
Translated 201/374 parameters...
Translated 202/374 parameters...
Translated 203/374 parameters...
Translated 204/374 parameters...
Translated 205/374 parameters...
Translated 206/374 parameters...
Translated 207/374 parameters...
Translated 208/374 parameters...
Translated 209/374 parameters...
Translated 210/374 parameters...
Translated 211/374 parameters...
Translated 212/374 parameters...
Translated 213/374 parameters...
Translated 214/374 parameters...
Translated 215/374 parameters...
Translated 216/374 parameters...
Translated 217/374 parameters...
Translated 218/374 parameters...
Translated 219/374 parameters...
Translated 220/374 parameters...
Translated 221/374 parameters...
Translated 222/374 parameters...
Translated 223/374 parameters...
Translated 224/374 parameters...
Translated 225/374 parameters...
Translated 226/374 parameters...
Translated 227/374 parameters...
Translated 228/374 parameters...
Translated 229/374 parameters...
Translated 230/374 parameters...
Translated 231/374 parameters...
Translated 232/374 parameters...
Translated 233/374 parameters...
Translated 234/374 parameters...
Translated 235/374 parameters...
Translated 236/374 parameters...
Translated 237/374 parameters...
Translated 238/374 parameters...
Translated 239/374 parameters...
Translated 240/374 parameters...
Translated 241/374 parameters...
Translated 242/374 parameters...
Translated 243/374 parameters...
Translated 244/374 parameters...
Translated 245/374 parameters...
Translated 246/374 parameters...
Translated 247/374 parameters...
Translated 248/374 parameters...
Translated 249/374 parameters...
Translated 250/374 parameters...
Translated 251/374 parameters...
Translated 252/374 parameters...
Translated 253/374 parameters...
Translated 254/374 parameters...
Translated 255/374 parameters...
Translated 256/374 parameters...
Translated 257/374 parameters...
Translated 258/374 parameters...
Translated 259/374 parameters...
Translated 260/374 parameters...
Translated 261/374 parameters...
Translated 262/374 parameters...
Translated 263/374 parameters...
Translated 264/374 parameters...
Translated 265/374 parameters...
Translated 266/374 parameters...
Translated 267/374 parameters...
Translated 268/374 parameters...
Translated 269/374 parameters...
Translated 270/374 parameters...
Translated 271/374 parameters...
Translated 272/374 parameters...
Translated 273/374 parameters...
Translated 274/374 parameters...
Translated 275/374 parameters...
Translated 276/374 parameters...
Translated 277/374 parameters...
Translated 278/374 parameters...
Translated 279/374 parameters...
Translated 280/374 parameters...
Translated 281/374 parameters...
Translated 282/374 parameters...
Translated 283/374 parameters...
Translated 284/374 parameters...
Translated 285/374 parameters...
Translated 286/374 parameters...
Translated 287/374 parameters...
Translated 288/374 parameters...
Translated 289/374 parameters...
Translated 290/374 parameters...
Translated 291/374 parameters...
Translated 292/374 parameters...
Translated 293/374 parameters...
Translated 294/374 parameters...
Translated 295/374 parameters...
Translated 296/374 parameters...
Translated 297/374 parameters...
Translated 298/374 parameters...
Translated 299/374 parameters...
Translated 300/374 parameters...
Translated 301/374 parameters...
Translated 302/374 parameters...
Translated 303/374 parameters...
Translated 304/374 parameters...
Translated 305/374 parameters...
Translated 306/374 parameters...
Translated 307/374 parameters...
Translated 308/374 parameters...
Translated 309/374 parameters...
Translated 310/374 parameters...
Translated 311/374 parameters...
Translated 312/374 parameters...
Translated 313/374 parameters...
Translated 314/374 parameters...
Translated 315/374 parameters...
Translated 316/374 parameters...
Translated 317/374 parameters...
Translated 318/374 parameters...
Translated 319/374 parameters...
Translated 320/374 parameters...
Translated 321/374 parameters...
Translated 322/374 parameters...
Translated 323/374 parameters...
Translated 324/374 parameters...
Translated 325/374 parameters...
Translated 326/374 parameters...
Translated 327/374 parameters...
Translated 328/374 parameters...
Translated 329/374 parameters...
Translated 330/374 parameters...
Translated 331/374 parameters...
Translated 332/374 parameters...
Translated 333/374 parameters...
Translated 334/374 parameters...
Translated 335/374 parameters...
Translated 336/374 parameters...
Translated 337/374 parameters...
Translated 338/374 parameters...
Translated 339/374 parameters...
Translated 340/374 parameters...
Translated 341/374 parameters...
Translated 342/374 parameters...
Translated 343/374 parameters...
Translated 344/374 parameters...
Translated 345/374 parameters...
Translated 346/374 parameters...
Translated 347/374 parameters...
Translated 348/374 parameters...
Translated 349/374 parameters...
Translated 350/374 parameters...
Translated 351/374 parameters...
Translated 352/374 parameters...
Translated 353/374 parameters...
Translated 354/374 parameters...
Translated 355/374 parameters...
Translated 356/374 parameters...
Translated 357/374 parameters...
Translated 358/374 parameters...
Translated 359/374 parameters...
Translated 360/374 parameters...
Translated 361/374 parameters...
Translated 362/374 parameters...
Translated 363/374 parameters...
Translated 364/374 parameters...
Translated 365/374 parameters...
Translated 366/374 parameters...
Translated 367/374 parameters...
Translated 368/374 parameters...
Translated 369/374 parameters...
Translated 370/374 parameters...
Translated 371/374 parameters...
Translated 372/374 parameters...
Translated 373/374 parameters...
Translated 374/374 parameters...
----------PASS 5: EVALUATE FUNCTION CALLS----------
----------PASS 6: CATEGORIZE PARAMETER VALUE MISMATCHES----------
Creating API backend for model ApiModel.Gpt5...
Created API backend for model gpt-5 with auto-retry on rate limits
API backend created successfully for model ApiModel.Gpt5
Categorized actual_value: "Âú£Âú∞‰∫öÂì•" and expected_values: ['"SD"', '"San Diego"', '"San Diego, CA"', '"CA"'] to category: LANGUAGE_MISMATCH_EXACTLY_SAME_MEANING
Categorized 1/32 errors...
Categorized actual_value: "The text is in Japanese characters but seems to be a mix of Chinese and Japanese. A possible translation is:\n\nBenefit Hole Poetry Ball Association" and expected_values: ['"Liverpool F.C."', '"Liverpool"'] to category: WRONG_VALUE
Categorized 2/32 errors...
Categorized actual_value: 2 and expected_values: ['2.5'] to category: RELEVANT_BUT_INCORRECT
Categorized 3/32 errors...
Categorized actual_value: -9 and expected_values: ['-9.81', '""'] to category: RELEVANT_BUT_INCORRECT
Categorized 4/32 errors...
Categorized actual_value: 0 and expected_values: ['5.0'] to category: WRONG_VALUE
Categorized 5/32 errors...
Categorized actual_value: "Chao Liu Zhong season 3" and expected_values: ['"Cyberpunk 2077"'] to category: WRONG_VALUE
Categorized 6/32 errors...
Categorized actual_value: ["macaroni","cheese"] and expected_values: ['["pasta","cheese"]', '["cheese","pasta"]'] to category: RELEVANT_BUT_INCORRECT
Categorized 7/32 errors...
Categorized actual_value: 9 and expected_values: ['9.8', '""'] to category: RELEVANT_BUT_INCORRECT
Categorized 8/32 errors...
Categorized actual_value: 0 and expected_values: ['0.95'] to category: WRONG_VALUE
Categorized 9/32 errors...
Categorized actual_value: 9 and expected_values: ['9.8'] to category: RELEVANT_BUT_INCORRECT
Categorized 10/32 errors...
Categorized actual_value: "housing prices" and expected_values: ['"house_price"', '"house price"'] to category: RELEVANT_BUT_INCORRECT
Categorized 11/32 errors...
Categorized actual_value: 0 and expected_values: ['0.01'] to category: RELEVANT_BUT_INCORRECT
Categorized 12/32 errors...
Categorized actual_value: "house prices" and expected_values: ['"house_price"', '"house price"'] to category: RELEVANT_BUT_INCORRECT
Categorized 13/32 errors...
Categorized actual_value: "Single Pet" and expected_values: ['"single"', '"Single"'] to category: RELEVANT_BUT_INCORRECT
Categorized 14/32 errors...
Categorized actual_value: 1 and expected_values: ['1.75'] to category: RELEVANT_BUT_INCORRECT
Categorized 15/32 errors...
Categorized actual_value: "Residence" and expected_values: ['"villa"'] to category: RELEVANT_BUT_INCORRECT
Categorized 16/32 errors...
Categorized actual_value: "Chulynne Medori" and expected_values: ['"Lionel Messi"'] to category: WRONG_VALUE
Categorized 17/32 errors...
Categorized actual_value: 0 and expected_values: ['0.08'] to category: RELEVANT_BUT_INCORRECT
Categorized 18/32 errors...
Categorized actual_value: ["interest rates","joblessness rates"] and expected_values: ['["interest rates","unemployment rates"]', '["interest_rate","unemployment_rate"]', '["interest rate","unemployment rate"]'] to category: EXACTLY_SAME_MEANING
Categorized 19/32 errors...
Categorized actual_value: "euro" and expected_values: ['"Euro"', '"EUR"'] to category: EXACTLY_SAME_MEANING
Categorized 20/32 errors...
Categorized actual_value: 4 and expected_values: ['4.5'] to category: RELEVANT_BUT_INCORRECT
Categorized 21/32 errors...
Categorized actual_value: ["fettuccine","cheese"] and expected_values: ['["pasta","cheese"]', '["cheese","pasta"]'] to category: RELEVANT_BUT_INCORRECT
Categorized 22/32 errors...
Categorized actual_value: 0 and expected_values: ['0.3'] to category: WRONG_VALUE
Categorized 23/32 errors...
Categorized actual_value: 12 and expected_values: ['"December"', '"Dec"'] to category: EXACTLY_SAME_MEANING
Categorized 24/32 errors...
Categorized actual_value: "Sengoku period" and expected_values: ['"Ice age"'] to category: WRONG_VALUE
Categorized 25/32 errors...
Categorized actual_value: 0 and expected_values: ['0.5'] to category: RELEVANT_BUT_INCORRECT
Categorized 26/32 errors...
Categorized actual_value: "Âçï‰∫∫ÂÆ†" and expected_values: ['"single"', '"Single"'] to category: LANGUAGE_MISMATCH_RELEVANT_BUT_INCORRECT
Categorized 27/32 errors...
Categorized actual_value: 0 and expected_values: ['0.05'] to category: RELEVANT_BUT_INCORRECT
Categorized 28/32 errors...
Categorized actual_value: ["behavioral psychology","group dynamics"] and expected_values: ['["behaviour","group dynamics"]', '["group dynamics","behaviour"]'] to category: RELEVANT_BUT_INCORRECT
Categorized 29/32 errors...
Categorized actual_value: ["interest levels","jobless rates"] and expected_values: ['["interest rates","unemployment rates"]', '["interest_rate","unemployment_rate"]', '["interest rate","unemployment rate"]'] to category: RELEVANT_BUT_INCORRECT
Categorized 30/32 errors...
Categorized actual_value: ["points per game","assists","minutes"] and expected_values: ['["points per game","assists","minutes per game"]', '["points per game","minutes per game","assists"]', '["assists","points per game","minutes per game"]', '["assists","minutes per game","points per game"]', '["minutes per game","points per game","assists"]', '["minutes per game","assists","points per game"]', '["points","assists","minutes"]', '["points","minutes","assists"]', '["assists","points","minutes"]', '["assists","minutes","points"]', '["minutes","points","assists"]', '["minutes","assists","points"]', '["points_per_game","assists","minutes_per_game"]', '["points_per_game","minutes_per_game","assists"]', '["assists","points_per_game","minutes_per_game"]', '["assists","minutes_per_game","points_per_game"]', '["minutes_per_game","points_per_game","assists"]', '["minutes_per_game","assists","points_per_game"]'] to category: RELEVANT_BUT_INCORRECT
Categorized 31/32 errors...
Categorized actual_value: ["average_points","average_assists","average_minutes"] and expected_values: ['["points per game","assists","minutes per game"]', '["points per game","minutes per game","assists"]', '["assists","points per game","minutes per game"]', '["assists","minutes per game","points per game"]', '["minutes per game","points per game","assists"]', '["minutes per game","assists","points per game"]', '["points","assists","minutes"]', '["points","minutes","assists"]', '["assists","points","minutes"]', '["assists","minutes","points"]', '["minutes","points","assists"]', '["minutes","assists","points"]', '["points_per_game","assists","minutes_per_game"]', '["points_per_game","minutes_per_game","assists"]', '["assists","points_per_game","minutes_per_game"]', '["assists","minutes_per_game","points_per_game"]', '["minutes_per_game","points_per_game","assists"]', '["minutes_per_game","assists","points_per_game"]'] to category: EXACTLY_SAME_MEANING
Categorized 32/32 errors...
----------PASS 7: GENERATE FINAL REPORT----------
ERROR 12-22 20:49:55 [core_client.py:600] Engine core proc EngineCore_DP0 died unexpectedly, shutting down client.
