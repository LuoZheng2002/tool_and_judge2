ðŸ”— Found pyo3 bindings
ðŸ Found CPython 3.13 at /projects/bfdz/zluo8/tool_and_judge2/.venv/bin/python
warning: unused imports: `Value` and `value`
 --> src/tool_category_cache.rs:3:18
  |
3 | use serde_json::{Value, value};
  |                  ^^^^^  ^^^^^
  |
  = note: `#[warn(unused_imports)]` on by default

warning: unused imports: `Deserialize` and `Serialize`
 --> src/models/model_interface.rs:4:13
  |
4 | use serde::{Deserialize, Serialize};
  |             ^^^^^^^^^^^  ^^^^^^^^^

warning: unused import: `any::Any`
 --> src/models/backend.rs:1:11
  |
1 | use std::{any::Any, collections::HashMap, sync::Arc};
  |           ^^^^^^^^

warning: unused import: `Bound`
 --> src/models/backend.rs:4:12
  |
4 | use pyo3::{Bound, pyclass};
  |            ^^^^^

warning: unused import: `PyDict`
 --> src/models/api_backend.rs:1:53
  |
1 | use pyo3::{Py, PyAny, Python, types::{PyAnyMethods, PyDict}};
  |                                                     ^^^^^^

warning: unused imports: `GenerationResult` and `ModelBackend`
 --> src/models/api_backend.rs:5:23
  |
5 |     models::backend::{GenerationResult, ModelBackend},
  |                       ^^^^^^^^^^^^^^^^  ^^^^^^^^^^^^

warning: unused imports: `Model` and `models::backend::ModelBackend`
 --> src/models/vllm_backend.rs:4:26
  |
4 |     config::{LocalModel, Model},
  |                          ^^^^^
5 |     models::backend::ModelBackend,
  |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

warning: unused import: `collections::HashMap`
 --> src/models/gpt5_interface.rs:1:21
  |
1 | use std::{any::Any, collections::HashMap, sync::Arc};
  |                     ^^^^^^^^^^^^^^^^^^^^

warning: unused import: `BfclGroundTruthFunctionCall`
  --> src/models/gpt5_interface.rs:11:26
   |
11 |         BfclFunctionDef, BfclGroundTruthFunctionCall, BfclOutputFunctionCall, BfclParameter,
   |                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^

warning: unused imports: `PyAny` and `Py`
  --> src/models/gpt5_interface.rs:17:12
   |
17 | use pyo3::{Py, PyAny, Python, types::PyAnyMethods};
   |            ^^  ^^^^^

warning: unused import: `serde_json::json`
  --> src/models/gpt5_interface.rs:22:5
   |
22 | use serde_json::json;
   |     ^^^^^^^^^^^^^^^^

warning: unused import: `any::Any`
 --> src/models/llama3_1_interface.rs:1:11
  |
1 | use std::{any::Any, sync::Arc};
  |           ^^^^^^^^

warning: unused import: `vllm_backend::VllmBackend`
 --> src/models/llama3_1_interface.rs:6:42
  |
6 |         model_interface::ModelInterface, vllm_backend::VllmBackend,
  |                                          ^^^^^^^^^^^^^^^^^^^^^^^^^

warning: unused import: `std::collections::HashMap`
 --> src/tool_bfcl_formats.rs:1:5
  |
1 | use std::collections::HashMap;
  |     ^^^^^^^^^^^^^^^^^^^^^^^^^

warning: unused import: `serde_json::json`
 --> src/tool_bfcl_formats.rs:5:5
  |
5 | use serde_json::json;
  |     ^^^^^^^^^^^^^^^^

warning: unused import: `Ordering`
  --> src/tool_run.rs:12:31
   |
12 |         atomic::{AtomicUsize, Ordering},
   |                               ^^^^^^^^

warning: unused import: `self`
  --> src/tool_run.rs:20:32
   |
20 |         function_name_mapper::{self, FunctionNameMapper},
   |                                ^^^^

warning: unused import: `BfclOutputFunctionCall`
 --> src/tool_evaluate.rs:5:49
  |
5 |         BfclDatasetEntry, BfclGroundTruthEntry, BfclOutputFunctionCall, BfclParameter,
  |                                                 ^^^^^^^^^^^^^^^^^^^^^^

warning: unused import: `tool_file_models::CategorizedEntry`
  --> src/tool_categorize.rs:18:5
   |
18 |     tool_file_models::CategorizedEntry,
   |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

warning: unused import: `std::fs::File`
 --> src/util.rs:1:5
  |
1 | use std::fs::File;
  |     ^^^^^^^^^^^^^

warning: unused import: `Bound`
  --> src/lib.rs:19:16
   |
19 |     use pyo3::{Bound, Py, pyfunction, types::PyList};
   |                ^^^^^

warning: unused variable: `total_entries`
   --> src/tool_run.rs:445:17
    |
445 |             let total_entries = inference_raw_entries.len();
    |                 ^^^^^^^^^^^^^ help: if this is intentional, prefix it with an underscore: `_total_entries`
    |
    = note: `#[warn(unused_variables)]` on by default

warning: unused variable: `model_interface`
  --> src/tool_categorize.rs:23:5
   |
23 |     model_interface: Arc<dyn ModelInterface>,
   |     ^^^^^^^^^^^^^^^ help: if this is intentional, prefix it with an underscore: `_model_interface`

warning: unused variable: `backend`
  --> src/tool_categorize.rs:24:5
   |
24 |     backend: Arc<ModelBackend>,
   |     ^^^^^^^ help: if this is intentional, prefix it with an underscore: `_backend`

warning: unused import: `prelude`
  --> src/models/gpt5_interface.rs:18:12
   |
18 | use pyo3::{prelude::*, types::PyList};
   |            ^^^^^^^

warning: unused import: `prelude`
  --> src/models/llama3_1_interface.rs:17:12
   |
17 | use pyo3::{prelude::*, types::PyList};
   |            ^^^^^^^

warning: `codebase_rs` (lib) generated 26 warnings (run `cargo fix --lib -p codebase_rs` to apply 21 suggestions)
    Finished `dev` profile [unoptimized + debuginfo] target(s) in 0.68s
ðŸ“– Found type stub file at codebase_rs.pyi
ðŸ“¦ Built wheel for CPython 3.13 to /tmp/.tmpn1no8b/codebase_rs-0.1.0-cp313-cp313-linux_x86_64.whl
ðŸ›  Installed codebase_rs-0.1.0

thread '<unnamed>' panicked at src/models/vllm_backend.rs:27:18:
Failed to call create_vllm_backend: PyErr { type: <class 'OSError'>, value: OSError('You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-6940ec05-221c9cca7d6d2a3853baca14;9f265ac7-b1b0-423f-8f76-53140e319450)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.'), traceback: Some("Traceback (most recent call last):\n  File \"/projects/bfdz/zluo8/tool_and_judge2/src_py/vllm_backend.py\", line 19, in create_vllm_backend\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n  File \"/projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/transformers/models/auto/tokenization_auto.py\", line 1109, in from_pretrained\n    config = AutoConfig.from_pretrained(\n  File \"/projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/transformers/models/auto/configuration_auto.py\", line 1332, in from_pretrained\n    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n  File \"/projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/transformers/configuration_utils.py\", line 662, in get_config_dict\n    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n  File \"/projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/transformers/configuration_utils.py\", line 721, in _get_config_dict\n    resolved_config_file = cached_file(\n  File \"/projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/transformers/utils/hub.py\", line 322, in cached_file\n    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)\n  File \"/projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/transformers/utils/hub.py\", line 543, in cached_files\n    raise OSError(\n") }
note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace
Traceback (most recent call last):
  File "/projects/bfdz/zluo8/tool_and_judge2/tool.py", line 47, in <module>
    asyncio.run(codebase_rs.tool_run_async(configs, args.num_gpus))
    ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sw/rh9.4/spack/v1.0.0/sw/linux-x86_64_v2/python-3.13.5-7bp2p4z/lib/python3.13/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ~~~~~~~~~~^^^^^^
  File "/sw/rh9.4/spack/v1.0.0/sw/linux-x86_64_v2/python-3.13.5-7bp2p4z/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^
  File "/sw/rh9.4/spack/v1.0.0/sw/linux-x86_64_v2/python-3.13.5-7bp2p4z/lib/python3.13/asyncio/base_events.py", line 725, in run_until_complete
    return future.result()
           ~~~~~~~~~~~~~^^
pyo3_runtime.PanicException: Failed to call create_vllm_backend: PyErr { type: <class 'OSError'>, value: OSError('You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-6940ec05-221c9cca7d6d2a3853baca14;9f265ac7-b1b0-423f-8f76-53140e319450)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.'), traceback: Some("Traceback (most recent call last):\n  File \"/projects/bfdz/zluo8/tool_and_judge2/src_py/vllm_backend.py\", line 19, in create_vllm_backend\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n  File \"/projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/transformers/models/auto/tokenization_auto.py\", line 1109, in from_pretrained\n    config = AutoConfig.from_pretrained(\n  File \"/projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/transformers/models/auto/configuration_auto.py\", line 1332, in from_pretrained\n    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n  File \"/projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/transformers/configuration_utils.py\", line 662, in get_config_dict\n    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n  File \"/projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/transformers/configuration_utils.py\", line 721, in _get_config_dict\n    resolved_config_file = cached_file(\n  File \"/projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/transformers/utils/hub.py\", line 322, in cached_file\n    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)\n  File \"/projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/transformers/utils/hub.py\", line 543, in cached_files\n    raise OSError(\n") }
