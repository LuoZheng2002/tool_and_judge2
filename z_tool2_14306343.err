ðŸ”— Found pyo3 bindings
ðŸ Found CPython 3.13 at /projects/bfdz/zluo8/tool_and_judge2/.venv/bin/python
warning: unused import: `backend::ModelBackend`
 --> src/models/model_interface.rs:6:9
  |
6 |         backend::ModelBackend,
  |         ^^^^^^^^^^^^^^^^^^^^^
  |
  = note: `#[warn(unused_imports)]` on by default

warning: unused import: `sync::Arc`
 --> src/models/deepseek_interface.rs:1:33
  |
1 | use std::{collections::HashMap, sync::Arc};
  |                                 ^^^^^^^^^

warning: unused import: `backend::ModelBackend`
 --> src/models/deepseek_interface.rs:5:9
  |
5 |         backend::ModelBackend, function_name_mapper::FunctionNameMapper,
  |         ^^^^^^^^^^^^^^^^^^^^^

warning: unused import: `std::sync::Arc`
 --> src/models/gpt5_interface.rs:1:5
  |
1 | use std::sync::Arc;
  |     ^^^^^^^^^^^^^^

warning: unused import: `backend::ModelBackend`
 --> src/models/gpt5_interface.rs:5:9
  |
5 |         backend::ModelBackend, function_name_mapper::FunctionNameMapper,
  |         ^^^^^^^^^^^^^^^^^^^^^

warning: unused import: `Python`
  --> src/models/gpt5_interface.rs:14:12
   |
14 | use pyo3::{Python, types::PyAnyMethods};
   |            ^^^^^^

warning: unused import: `std::sync::Arc`
 --> src/models/llama3_1_interface.rs:1:5
  |
1 | use std::sync::Arc;
  |     ^^^^^^^^^^^^^^

warning: unused import: `backend::ModelBackend`
 --> src/models/llama3_1_interface.rs:5:9
  |
5 |         backend::ModelBackend, function_name_mapper::FunctionNameMapper,
  |         ^^^^^^^^^^^^^^^^^^^^^

warning: unused import: `Python`
  --> src/models/llama3_1_interface.rs:14:12
   |
14 | use pyo3::{Python, types::PyAnyMethods};
   |            ^^^^^^

warning: unused import: `types::PyAnyMethods`
  --> src/models/llama3_1_interface.rs:14:20
   |
14 | use pyo3::{Python, types::PyAnyMethods};
   |                    ^^^^^^^^^^^^^^^^^^^

warning: unused import: `types::PyAnyMethods`
  --> src/models/gpt5_interface.rs:14:20
   |
14 | use pyo3::{Python, types::PyAnyMethods};
   |                    ^^^^^^^^^^^^^^^^^^^

warning: `codebase_rs` (lib) generated 11 warnings (run `cargo fix --lib -p codebase_rs` to apply 9 suggestions)
    Finished `release` profile [optimized] target(s) in 3.73s
ðŸ“– Found type stub file at codebase_rs.pyi
ðŸ“¦ Built wheel for CPython 3.13 to /tmp/.tmptExcm3/codebase_rs-0.1.0-cp313-cp313-linux_x86_64.whl
ðŸ›  Installed codebase_rs-0.1.0
/projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/transformers/models/auto/tokenization_auto.py:1041: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[rank2]:[W1222 16:15:48.956856217 TCPStore.cpp:125] [c10d] recvValue failed on SocketImpl(fd=161, addr=[localhost]:53702, remote=[localhost]:58519): Failed to recv, got 0 bytes. Connection was likely closed. Did the remote server shutdown or crash?
Exception raised from recvBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:697 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x7feb9b5f4b80 in /projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x5ffd531 (0x7febdd9fd531 in /projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/torch/lib/libtorch_cpu.so)
frame #2: <unknown function> + 0x5ffe92d (0x7febdd9fe92d in /projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/torch/lib/libtorch_cpu.so)
frame #3: <unknown function> + 0x5fff4da (0x7febdd9ff4da in /projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x31e (0x7febdd9fa1fe in /projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::ProcessGroupNCCL::HeartbeatMonitor::runLoop() + 0x3c8 (0x7feb9c5776b8 in /projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0xdbad4 (0x7febfa743ad4 in /lib64/libstdc++.so.6)
frame #7: <unknown function> + 0x89c0a (0x7fec0e4e6c0a in /lib64/libc.so.6)
frame #8: <unknown function> + 0x10ec60 (0x7fec0e56bc60 in /lib64/libc.so.6)

[rank2]:[W1222 16:15:48.081083361 ProcessGroupNCCL.cpp:1771] [PG ID 0 PG GUID 0 Rank 2] Failed to check the "should dump" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: Failed to recv, got 0 bytes. Connection was likely closed. Did the remote server shutdown or crash?
[rank2]:[W1222 16:15:49.081287276 TCPStore.cpp:106] [c10d] sendBytes failed on SocketImpl(fd=161, addr=[localhost]:53702, remote=[localhost]:58519): Broken pipe
Exception raised from sendBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:668 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x7feb9b5f4b80 in /projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x5ffd531 (0x7febdd9fd531 in /projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/torch/lib/libtorch_cpu.so)
frame #2: <unknown function> + 0x5ffddc2 (0x7febdd9fddc2 in /projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/torch/lib/libtorch_cpu.so)
frame #3: <unknown function> + 0x5fff8ce (0x7febdd9ff8ce in /projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x30e (0x7febdd9fa1ee in /projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::ProcessGroupNCCL::HeartbeatMonitor::runLoop() + 0x3c8 (0x7feb9c5776b8 in /projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0xdbad4 (0x7febfa743ad4 in /lib64/libstdc++.so.6)
frame #7: <unknown function> + 0x89c0a (0x7fec0e4e6c0a in /lib64/libc.so.6)
frame #8: <unknown function> + 0x10ec60 (0x7fec0e56bc60 in /lib64/libc.so.6)

[rank2]:[W1222 16:15:49.084165997 ProcessGroupNCCL.cpp:1771] [PG ID 0 PG GUID 0 Rank 2] Failed to check the "should dump" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: Broken pipe
[rank2]:[W1222 16:15:50.084301119 TCPStore.cpp:106] [c10d] sendBytes failed on SocketImpl(fd=161, addr=[localhost]:53702, remote=[localhost]:58519): Broken pipe
Exception raised from sendBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:668 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x7feb9b5f4b80 in /projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x5ffd531 (0x7febdd9fd531 in /projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/torch/lib/libtorch_cpu.so)
frame #2: <unknown function> + 0x5ffddc2 (0x7febdd9fddc2 in /projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/torch/lib/libtorch_cpu.so)
frame #3: <unknown function> + 0x5fff8ce (0x7febdd9ff8ce in /projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x30e (0x7febdd9fa1ee in /projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::ProcessGroupNCCL::HeartbeatMonitor::runLoop() + 0x3c8 (0x7feb9c5776b8 in /projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0xdbad4 (0x7febfa743ad4 in /lib64/libstdc++.so.6)
frame #7: <unknown function> + 0x89c0a (0x7fec0e4e6c0a in /lib64/libc.so.6)
frame #8: <unknown function> + 0x10ec60 (0x7fec0e56bc60 in /lib64/libc.so.6)

[rank2]:[W1222 16:15:50.087213251 ProcessGroupNCCL.cpp:1771] [PG ID 0 PG GUID 0 Rank 2] Failed to check the "should dump" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: Broken pipe
[0;36m(EngineCore_DP0 pid=177473)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=177473)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=177473)[0;0m   File "/sw/rh9.4/spack/v1.0.0/sw/linux-x86_64_v2/python-3.13.5-7bp2p4z/lib/python3.13/multiprocessing/process.py", line 313, in _bootstrap
[0;36m(EngineCore_DP0 pid=177473)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=177473)[0;0m     ~~~~~~~~^^
[0;36m(EngineCore_DP0 pid=177473)[0;0m   File "/sw/rh9.4/spack/v1.0.0/sw/linux-x86_64_v2/python-3.13.5-7bp2p4z/lib/python3.13/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=177473)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=177473)[0;0m     ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=177473)[0;0m   File "/projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 847, in run_engine_core
[0;36m(EngineCore_DP0 pid=177473)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=177473)[0;0m   File "/projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 834, in run_engine_core
[0;36m(EngineCore_DP0 pid=177473)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=177473)[0;0m   File "/projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 610, in __init__
[0;36m(EngineCore_DP0 pid=177473)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=177473)[0;0m     ~~~~~~~~~~~~~~~~^
[0;36m(EngineCore_DP0 pid=177473)[0;0m         vllm_config, executor_class, log_stats, executor_fail_callback
[0;36m(EngineCore_DP0 pid=177473)[0;0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=177473)[0;0m     )
[0;36m(EngineCore_DP0 pid=177473)[0;0m     ^
[0;36m(EngineCore_DP0 pid=177473)[0;0m   File "/projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[0;36m(EngineCore_DP0 pid=177473)[0;0m     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=177473)[0;0m                           ~~~~~~~~~~~~~~^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=177473)[0;0m   File "/projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[0;36m(EngineCore_DP0 pid=177473)[0;0m     super().__init__(vllm_config)
[0;36m(EngineCore_DP0 pid=177473)[0;0m     ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=177473)[0;0m   File "/projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=177473)[0;0m     self._init_executor()
[0;36m(EngineCore_DP0 pid=177473)[0;0m     ~~~~~~~~~~~~~~~~~~~^^
[0;36m(EngineCore_DP0 pid=177473)[0;0m   File "/projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/vllm/v1/executor/multiproc_executor.py", line 174, in _init_executor
[0;36m(EngineCore_DP0 pid=177473)[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)
[0;36m(EngineCore_DP0 pid=177473)[0;0m                    ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=177473)[0;0m   File "/projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/vllm/v1/executor/multiproc_executor.py", line 660, in wait_for_ready
[0;36m(EngineCore_DP0 pid=177473)[0;0m     raise e from None
[0;36m(EngineCore_DP0 pid=177473)[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
/sw/rh9.4/spack/v1.0.0/sw/linux-x86_64_v2/python-3.13.5-7bp2p4z/lib/python3.13/multiprocessing/resource_tracker.py:301: UserWarning: resource_tracker: There appear to be 3 leaked shared_memory objects to clean up at shutdown: {'/psm_29e0c397', '/psm_8e26af67', '/psm_cfa1c01e'}
  warnings.warn(
Traceback (most recent call last):
  File "/projects/bfdz/zluo8/tool_and_judge2/tool.py", line 163, in <module>
    asyncio.run(collect_all_question_translations_async(question_entries))
    ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sw/rh9.4/spack/v1.0.0/sw/linux-x86_64_v2/python-3.13.5-7bp2p4z/lib/python3.13/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ~~~~~~~~~~^^^^^^
  File "/sw/rh9.4/spack/v1.0.0/sw/linux-x86_64_v2/python-3.13.5-7bp2p4z/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^
  File "/sw/rh9.4/spack/v1.0.0/sw/linux-x86_64_v2/python-3.13.5-7bp2p4z/lib/python3.13/asyncio/base_events.py", line 725, in run_until_complete
    return future.result()
           ~~~~~~~~~~~~~^^
  File "/projects/bfdz/zluo8/tool_and_judge2/tool.py", line 146, in collect_all_question_translations_async
    main_client, main_engine, main_tokenizer, main_is_api = create_backend(config.model)
                                                            ~~~~~~~~~~~~~~^^^^^^^^^^^^^^
  File "/projects/bfdz/zluo8/tool_and_judge2/tool.py", line 75, in create_backend
    engine, tokenizer = create_vllm_backend(local_model, num_gpus=args.num_gpus)
                        ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/projects/bfdz/zluo8/tool_and_judge2/src_py/vllm_backend.py", line 37, in create_vllm_backend
    engine = AsyncLLMEngine.from_engine_args(engine_args)
  File "/projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/vllm/v1/engine/async_llm.py", line 252, in from_engine_args
    return cls(
        vllm_config=vllm_config,
    ...<5 lines>...
        stat_loggers=stat_loggers,
    )
  File "/projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/vllm/v1/engine/async_llm.py", line 134, in __init__
    self.engine_core = EngineCoreClient.make_async_mp_client(
                       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        vllm_config=vllm_config,
        ^^^^^^^^^^^^^^^^^^^^^^^^
    ...<4 lines>...
        client_index=client_index,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/vllm/v1/engine/core_client.py", line 121, in make_async_mp_client
    return AsyncMPClient(*client_args)
  File "/projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/vllm/v1/engine/core_client.py", line 810, in __init__
    super().__init__(
    ~~~~~~~~~~~~~~~~^
        asyncio_mode=True,
        ^^^^^^^^^^^^^^^^^^
    ...<3 lines>...
        client_addresses=client_addresses,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/vllm/v1/engine/core_client.py", line 471, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sw/rh9.4/spack/v1.0.0/sw/linux-x86_64_v2/python-3.13.5-7bp2p4z/lib/python3.13/contextlib.py", line 148, in __exit__
    next(self.gen)
    ~~~~^^^^^^^^^^
  File "/projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
    ~~~~~~~~~~~~~~~~~~~~~~~^
        handshake_socket,
        ^^^^^^^^^^^^^^^^^
    ...<5 lines>...
        coordinator.proc if coordinator else None,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/projects/bfdz/zluo8/tool_and_judge2/.venv/lib/python3.13/site-packages/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
    ...<3 lines>...
    )
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
